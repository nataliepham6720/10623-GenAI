{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzDKYSq3o8NuK8vsyZt3VP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c24f465c37714854b78b39e4e6e704fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5e7e829642848a883f759083c2cd614",
              "IPY_MODEL_49a12c934a0948d18a98cbdf16f13876"
            ],
            "layout": "IPY_MODEL_f1ac3187031d40d4962c1264148f204a"
          }
        },
        "b5e7e829642848a883f759083c2cd614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84232e5e50144a448798312aa68073ae",
            "placeholder": "​",
            "style": "IPY_MODEL_968e25a5a49b45beae7171d14e7bcaa4",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "49a12c934a0948d18a98cbdf16f13876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97f5a3eba42642e1b7b91f51429b1151",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c3baf5ebc12421fa991ddd0b423eb18",
            "value": 1
          }
        },
        "f1ac3187031d40d4962c1264148f204a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84232e5e50144a448798312aa68073ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "968e25a5a49b45beae7171d14e7bcaa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97f5a3eba42642e1b7b91f51429b1151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c3baf5ebc12421fa991ddd0b423eb18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nataliepham6720/10623-GenAI/blob/main/HW0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iUY2z9cUQqx",
        "outputId": "dc3ea89a-8138-42a2-d249-7d0d82f774b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/10623/hw0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNB6dcwpZOhQ",
        "outputId": "d5ec360c-96ce-42bc-e8b1-457ee3a447d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/10623/hw0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myw_Lbl5ZJr9",
        "outputId": "030fa630-bfb8-4731-8c5f-2a265d88d853"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Copy of S24 10-423 623 Recitation 0.ipynb'   HW0.ipynb              model.pth\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/                                        image_classifier.txt   txt_classifier.py\n",
            " Homework-0.pdf                               img_classifiery.py     \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import necessary packages"
      ],
      "metadata": {
        "id": "Nse-c7NdUhbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install wandb\n",
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "DUeO0ea8VJHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12b47a2-1b40-464e-bc40-471d5d3ac37c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import tee\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.io import read_image"
      ],
      "metadata": {
        "id": "NG1VidL1Uf5-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification"
      ],
      "metadata": {
        "id": "LuD5SsIAUy2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import argparse\n",
        "\n",
        "import wandb\n",
        "\n",
        "# API Key is in your wandb settings\n",
        "wandb.login(key=\"e60f8238b8524c195edde224c6b3f3e645648586\")\n",
        "\n",
        "img_size = (256,256)\n",
        "num_labels = 3\n",
        "\n",
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "class CsvImageDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= self.__len__(): raise IndexError()\n",
        "        img_name = self.data_frame.loc[idx, \"image\"]\n",
        "        image = Image.open(img_name).convert(\"RGB\")  # Assuming RGB images\n",
        "        label = self.data_frame.loc[idx, \"label_idx\"]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def get_data(batch_size):\n",
        "    transform_img = T.Compose([\n",
        "        # T.Grayscale(num_output_channels=1),  # Convert to single-channel grayscale image\n",
        "        T.ToTensor(),\n",
        "        # T.Resize((28,28)),  # Resize to tiny image of size 28 x 28\n",
        "        T.Resize(min(img_size[0], img_size[1]), antialias=True),  # Resize the smallest side to 256 pixels\n",
        "        T.CenterCrop(img_size),  # Center crop to 256x256\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize each color dimension\n",
        "        # T.Normalize(mean=[0.485], std=[0.229])  # Normalize single-channel grayscale image\n",
        "        ])\n",
        "\n",
        "    train_data = CsvImageDataset(\n",
        "        csv_file='./data/img_train.csv',\n",
        "        transform=transform_img,\n",
        "    )\n",
        "    val_data = CsvImageDataset(\n",
        "        csv_file='./data/img_val.csv',\n",
        "        transform=transform_img,\n",
        "    )\n",
        "    test_data = CsvImageDataset(\n",
        "        csv_file='./data/img_test.csv',\n",
        "        transform=transform_img,\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    val_dataloader = DataLoader(val_data, batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    for X, y in train_dataloader:\n",
        "        print(f\"Shape of X [B, C, H, W]: {X.shape}\")\n",
        "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "        break\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        # First layer input size must be the dimension of the image\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            # nn.Linear(img_size[0] * img_size[1], 512), # for grayscale input\n",
        "            # nn.Linear(28 * 28 * 3, 512), # for tiny input\n",
        "            nn.Linear(img_size[0] * img_size[1] * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        #self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(128 * 64 * 64, 512)\n",
        "        self.fc2 = nn.Linear(512, num_labels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        # print(x.size())\n",
        "        #x = self.relu(self.conv3(x))\n",
        "        #x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def train_one_epoch(dataloader, model, loss_fn, optimizer, t):\n",
        "    size = len(dataloader.dataset)\n",
        "    batch_size = dataloader.batch_size\n",
        "    model.train()\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = loss.item() / batch_size\n",
        "        current = (batch + 1) * dataloader.batch_size\n",
        "        if batch % 10 == 0:\n",
        "            print(f\"Train loss = {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def evaluate(dataloader, dataname, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    avg_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            avg_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    avg_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"{dataname} accuracy = {(100*correct):>0.1f}%, {dataname} avg loss = {avg_loss:>8f}\")\n",
        "    return correct, avg_loss\n",
        "\n",
        "# Assuming model_predictions contains the predictions of the model for the first batch\n",
        "def log_images_with_labels(images, labels, model_predictions, dataset_type):\n",
        "    for i in range(len(images)):\n",
        "        predicted_label = model_predictions[i]\n",
        "        true_label = labels[i]\n",
        "        caption = f\"{predicted_label} / {true_label}\"\n",
        "\n",
        "        # Log image with caption to wandb\n",
        "        wandb.log({f\"{dataset_type}_image_{i}\": [wandb.Image(images[i], caption=caption)]})\n",
        "\n",
        "def main(n_epochs, batch_size, learning_rate):\n",
        "    print(f\"Using {device} device\")\n",
        "    train_dataloader, val_dataloader, test_dataloader = get_data(batch_size)\n",
        "\n",
        "    model = CNN().to(device)\n",
        "    print(model)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    # optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate) # Adam optimizer with momentum\n",
        "\n",
        "    # Initialize wandb run\n",
        "    run = wandb.init(\n",
        "        name = \"CNN_sgd_1\", # Enter useful info to prevent confusions\n",
        "        reinit = True,\n",
        "        project = \"10-423-623 HW0\", # Runs under the same project can be plotted together\n",
        "    )\n",
        "\n",
        "    # Save your network architecture as txt\n",
        "    #network_architecture = open(\"image_classifier.txt\", \"w\")\n",
        "    #network_architecture.write(str(model))\n",
        "    #network_architecture.close()\n",
        "\n",
        "    # Log this file into wandb (or any other files you want to save in a similar manner)\n",
        "    #wandb.save('image_classifier.txt')\n",
        "\n",
        "\n",
        "    for t in range(n_epochs):\n",
        "        print(f\"\\nEpoch {t+1}\\n-------------------------------\")\n",
        "        train_one_epoch(train_dataloader, model, loss_fn, optimizer, t)\n",
        "        epoch_train_acc, epoch_train_loss = evaluate(train_dataloader, \"Train\", model, loss_fn)\n",
        "        epoch_val_acc, epoch_val_loss = evaluate(val_dataloader, \"Validation\", model, loss_fn)\n",
        "\n",
        "        # Log the loss and accuracy in wandb as well\n",
        "        wandb.log({\"Loss\": epoch_val_loss, \"Accuracy\": epoch_val_acc})\n",
        "\n",
        "        # Output test loss and accuracy\n",
        "        epoch_test_acc, epoch_test_loss = evaluate(test_dataloader, \"Test\", model, loss_fn)\n",
        "\n",
        "        # Uncomment to log the first batch of each of the datasets (train, val, test), on the last epoch only, images with a caption\n",
        "        #class_names = [\"parrot\",\"narwhal\",\"axolotl\"]\n",
        "        #dataset_types = ['train', 'val', 'test']\n",
        "        #logs = [train_dataloader, val_dataloader, test_dataloader]\n",
        "        # Log images with labels for the last epoch of testing\n",
        "        #if t == n_epochs - 1:\n",
        "        #    for i, ds in enumerate(logs):\n",
        "        #        for batch, (X, y) in enumerate(ds):\n",
        "        #            if batch == 0:\n",
        "        #                X, y = X.to(device), y.to(device)\n",
        "        #                pred = model(X)\n",
        "        #                predicted_labels = [class_names[pred.item()] for pred in pred.argmax(1)]\n",
        "        #                true_labels = [class_names[label.item()] for label in y]\n",
        "        #                log_images_with_labels(X, true_labels, predicted_labels, dataset_type=dataset_types[i])\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"model.pth\")\n",
        "    #wandb.save('model.pth')\n",
        "    print(\"Saved PyTorch Model State to model.pth\")\n",
        "\n",
        "    # Load the model (just for the sake of example)\n",
        "    #model = NeuralNetwork().to(device)\n",
        "    #model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "    # Finish your wandb run\n",
        "    run.finish()"
      ],
      "metadata": {
        "id": "qbL1P9yiWXGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a23297-f125-40f7-8b33-1ffed390dfbd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if __name__ == '__main__':\n",
        "#    parser = argparse.ArgumentParser(description = 'Image Classifier')\n",
        "#    parser.add_argument('--n_epochs', default=5, help='The number of training epochs', type=int)\n",
        "#    parser.add_argument('--batch_size', default=8, help='The batch size', type=int)\n",
        "#    parser.add_argument('--learning_rate', default=1e-3, help='The learning rate for the optimizer', type=float)\n",
        "#\n",
        "#    args = parser.parse_args()\n",
        "#\n",
        "#    main(args.n_epochs, args.batch_size, args.learning_rate)"
      ],
      "metadata": {
        "id": "DL5_7PxDW5yO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(n_epochs=10, batch_size=8, learning_rate=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c24f465c37714854b78b39e4e6e704fe",
            "b5e7e829642848a883f759083c2cd614",
            "49a12c934a0948d18a98cbdf16f13876",
            "f1ac3187031d40d4962c1264148f204a",
            "84232e5e50144a448798312aa68073ae",
            "968e25a5a49b45beae7171d14e7bcaa4",
            "97f5a3eba42642e1b7b91f51429b1151",
            "9c3baf5ebc12421fa991ddd0b423eb18"
          ]
        },
        "id": "AhmGcNU1XezV",
        "outputId": "c4625dff-795e-4150-f5f6-41ebfb80520a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Shape of X [B, C, H, W]: torch.Size([8, 3, 256, 256])\n",
            "Shape of y: torch.Size([8]) torch.int64\n",
            "CNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=524288, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/10623/hw0/wandb/run-20240203_200340-s5ohmjuq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/s4/10-423-623%20HW0/runs/s5ohmjuq' target=\"_blank\">CNN_sgd_1</a></strong> to <a href='https://wandb.ai/s4/10-423-623%20HW0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/s4/10-423-623%20HW0' target=\"_blank\">https://wandb.ai/s4/10-423-623%20HW0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/s4/10-423-623%20HW0/runs/s5ohmjuq' target=\"_blank\">https://wandb.ai/s4/10-423-623%20HW0/runs/s5ohmjuq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Train loss = 0.136284  [    8/  540]\n",
            "Train loss = 0.136572  [   88/  540]\n",
            "Train loss = 0.124125  [  168/  540]\n",
            "Train loss = 0.112861  [  248/  540]\n",
            "Train loss = 0.094165  [  328/  540]\n",
            "Train loss = 0.194504  [  408/  540]\n",
            "Train loss = 0.140522  [  488/  540]\n",
            "Train accuracy = 61.9%, Train avg loss = 0.102718\n",
            "Validation accuracy = 62.1%, Validation avg loss = 0.103682\n",
            "Test accuracy = 50.4%, Test avg loss = 0.120782\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train loss = 0.113140  [    8/  540]\n",
            "Train loss = 0.107660  [   88/  540]\n",
            "Train loss = 0.086344  [  168/  540]\n",
            "Train loss = 0.087402  [  248/  540]\n",
            "Train loss = 0.081804  [  328/  540]\n",
            "Train loss = 0.210169  [  408/  540]\n",
            "Train loss = 0.126671  [  488/  540]\n",
            "Train accuracy = 66.3%, Train avg loss = 0.089240\n",
            "Validation accuracy = 65.5%, Validation avg loss = 0.095579\n",
            "Test accuracy = 53.0%, Test avg loss = 0.117076\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train loss = 0.098224  [    8/  540]\n",
            "Train loss = 0.092327  [   88/  540]\n",
            "Train loss = 0.078638  [  168/  540]\n",
            "Train loss = 0.072275  [  248/  540]\n",
            "Train loss = 0.078643  [  328/  540]\n",
            "Train loss = 0.203137  [  408/  540]\n",
            "Train loss = 0.108560  [  488/  540]\n",
            "Train accuracy = 75.0%, Train avg loss = 0.076079\n",
            "Validation accuracy = 68.1%, Validation avg loss = 0.089669\n",
            "Test accuracy = 58.3%, Test avg loss = 0.114864\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train loss = 0.087536  [    8/  540]\n",
            "Train loss = 0.083509  [   88/  540]\n",
            "Train loss = 0.073513  [  168/  540]\n",
            "Train loss = 0.060047  [  248/  540]\n",
            "Train loss = 0.075021  [  328/  540]\n",
            "Train loss = 0.191573  [  408/  540]\n",
            "Train loss = 0.091712  [  488/  540]\n",
            "Train accuracy = 80.2%, Train avg loss = 0.065790\n",
            "Validation accuracy = 68.1%, Validation avg loss = 0.087162\n",
            "Test accuracy = 60.0%, Test avg loss = 0.116199\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train loss = 0.079455  [    8/  540]\n",
            "Train loss = 0.074814  [   88/  540]\n",
            "Train loss = 0.068604  [  168/  540]\n",
            "Train loss = 0.050614  [  248/  540]\n",
            "Train loss = 0.070467  [  328/  540]\n",
            "Train loss = 0.179096  [  408/  540]\n",
            "Train loss = 0.077003  [  488/  540]\n",
            "Train accuracy = 85.6%, Train avg loss = 0.056169\n",
            "Validation accuracy = 69.0%, Validation avg loss = 0.084824\n",
            "Test accuracy = 63.5%, Test avg loss = 0.116919\n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train loss = 0.071740  [    8/  540]\n",
            "Train loss = 0.064421  [   88/  540]\n",
            "Train loss = 0.063952  [  168/  540]\n",
            "Train loss = 0.042282  [  248/  540]\n",
            "Train loss = 0.066337  [  328/  540]\n",
            "Train loss = 0.164554  [  408/  540]\n",
            "Train loss = 0.064459  [  488/  540]\n",
            "Train accuracy = 87.8%, Train avg loss = 0.047728\n",
            "Validation accuracy = 70.7%, Validation avg loss = 0.083764\n",
            "Test accuracy = 61.7%, Test avg loss = 0.117912\n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Train loss = 0.064215  [    8/  540]\n",
            "Train loss = 0.051775  [   88/  540]\n",
            "Train loss = 0.056471  [  168/  540]\n",
            "Train loss = 0.035866  [  248/  540]\n",
            "Train loss = 0.061110  [  328/  540]\n",
            "Train loss = 0.148169  [  408/  540]\n",
            "Train loss = 0.053252  [  488/  540]\n",
            "Train accuracy = 89.3%, Train avg loss = 0.040862\n",
            "Validation accuracy = 70.7%, Validation avg loss = 0.084154\n",
            "Test accuracy = 62.6%, Test avg loss = 0.120483\n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Train loss = 0.055547  [    8/  540]\n",
            "Train loss = 0.041352  [   88/  540]\n",
            "Train loss = 0.048645  [  168/  540]\n",
            "Train loss = 0.029615  [  248/  540]\n",
            "Train loss = 0.054527  [  328/  540]\n",
            "Train loss = 0.133757  [  408/  540]\n",
            "Train loss = 0.043409  [  488/  540]\n",
            "Train accuracy = 91.7%, Train avg loss = 0.035392\n",
            "Validation accuracy = 68.1%, Validation avg loss = 0.085827\n",
            "Test accuracy = 60.9%, Test avg loss = 0.124379\n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Train loss = 0.047050  [    8/  540]\n",
            "Train loss = 0.031872  [   88/  540]\n",
            "Train loss = 0.039609  [  168/  540]\n",
            "Train loss = 0.024380  [  248/  540]\n",
            "Train loss = 0.048887  [  328/  540]\n",
            "Train loss = 0.119314  [  408/  540]\n",
            "Train loss = 0.035195  [  488/  540]\n",
            "Train accuracy = 93.1%, Train avg loss = 0.030637\n",
            "Validation accuracy = 69.8%, Validation avg loss = 0.088186\n",
            "Test accuracy = 60.0%, Test avg loss = 0.129375\n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Train loss = 0.038688  [    8/  540]\n",
            "Train loss = 0.023970  [   88/  540]\n",
            "Train loss = 0.031275  [  168/  540]\n",
            "Train loss = 0.020035  [  248/  540]\n",
            "Train loss = 0.044571  [  328/  540]\n",
            "Train loss = 0.106779  [  408/  540]\n",
            "Train loss = 0.027731  [  488/  540]\n",
            "Train accuracy = 94.1%, Train avg loss = 0.026732\n",
            "Validation accuracy = 70.7%, Validation avg loss = 0.090892\n",
            "Test accuracy = 60.0%, Test avg loss = 0.135570\n",
            "Done!\n",
            "Saved PyTorch Model State to model.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c24f465c37714854b78b39e4e6e704fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▄▆▆▇██▆▇█</td></tr><tr><td>Loss</td><td>█▅▃▂▁▁▁▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.7069</td></tr><tr><td>Loss</td><td>0.09089</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CNN_sgd_1</strong> at: <a href='https://wandb.ai/s4/10-423-623%20HW0/runs/s5ohmjuq' target=\"_blank\">https://wandb.ai/s4/10-423-623%20HW0/runs/s5ohmjuq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240203_200340-s5ohmjuq/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification"
      ],
      "metadata": {
        "id": "_WlC5P10U7v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import time\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import wandb\n",
        "\n",
        "# API Key is in your wandb settings\n",
        "wandb.login(key=\"e60f8238b8524c195edde224c6b3f3e645648586\")\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 5  # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 8  # batch size for training\n",
        "EMBED_DIM = 64 # embedding size in model\n",
        "HIDDEN_DIM = 128 # hidden size in model\n",
        "MAX_LEN = 1024 # maximum text input length\n",
        "\n",
        "# Get cpu, gpu device for training.\n",
        "# mps does not (yet) support nn.EmbeddingBag.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "class CsvTextDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= self.__len__(): raise IndexError()\n",
        "        text = self.data_frame.loc[idx, \"article\"]\n",
        "        label = self.data_frame.loc[idx, \"label_idx\"]\n",
        "\n",
        "        if self.transform:\n",
        "            text = self.transform(text)\n",
        "\n",
        "        return text, label\n",
        "\n",
        "class CorpusInfo():\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.oov_token = '<UNK>'\n",
        "        self.pad_token = '<PAD>'\n",
        "\n",
        "        def yield_tokens(data_iter):\n",
        "            for text, _ in data_iter:\n",
        "                yield tokenizer(text)\n",
        "        self.vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[self.oov_token, self.pad_token])\n",
        "        self.vocab.set_default_index(self.vocab[self.oov_token])\n",
        "\n",
        "        self.oov_idx = self.vocab[self.oov_token]\n",
        "        self.pad_idx = self.vocab[self.pad_token]\n",
        "\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.num_labels = len(set([label for (text, label) in dataset]))\n",
        "\n",
        "class TextTransform(torch.Callable):\n",
        "    def __init__(self, tokenizer, vocab):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def tokenize_and_numericalize(self, text):\n",
        "        tokens = self.tokenizer(text)\n",
        "        return [self.vocab[token] for token in tokens]\n",
        "\n",
        "    def __call__(self, text):\n",
        "        return self.tokenize_and_numericalize(text)\n",
        "\n",
        "class MaxLen(torch.Callable):\n",
        "    def __init__(self, max_len):\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if len(x) > self.max_len:\n",
        "            x = x[:self.max_len]\n",
        "        return x\n",
        "\n",
        "class PadSequence(torch.Callable):\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        def to_int_tensor(x):\n",
        "            return torch.from_numpy(np.array(x, dtype=np.int64, copy=False))\n",
        "        # Convert each sequence of tokens to a Tensor\n",
        "        sequences = [to_int_tensor(x[0]) for x in batch]\n",
        "        # Convert the full sequence of labels to a Tensor\n",
        "        labels = to_int_tensor([x[1] for x in batch])\n",
        "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
        "        return sequences_padded, labels\n",
        "\n",
        "def get_data():\n",
        "    train_data = CsvTextDataset(\n",
        "        csv_file='./data/txt_train.csv',\n",
        "        transform=None,\n",
        "    )\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    corpus_info = CorpusInfo(train_data, tokenizer)\n",
        "    # print(corpus_info)\n",
        "\n",
        "    lengths = []\n",
        "    for X, y in train_dataloader:\n",
        "        # Assuming X contains the text data, calculate the length of each text\n",
        "        for text in X:\n",
        "            lengths.append(len(text.split()))  # Assuming the text is tokenized by whitespace\n",
        "\n",
        "        # print(X[-1])\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(lengths, bins=50, color='blue', edgecolor='black')\n",
        "    plt.title('Histogram of News Article Lengths')\n",
        "    plt.xlabel('Length of News Articles')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    transform_txt = T.Compose([\n",
        "        TextTransform(corpus_info.tokenizer, corpus_info.vocab),\n",
        "        MaxLen(MAX_LEN),\n",
        "    ])\n",
        "    train_data = CsvTextDataset(\n",
        "        csv_file='./data/txt_train.csv',\n",
        "        transform=transform_txt,\n",
        "    )\n",
        "    val_data = CsvTextDataset(\n",
        "        csv_file='./data/txt_val.csv',\n",
        "        transform=transform_txt,\n",
        "    )\n",
        "    test_data = CsvTextDataset(\n",
        "        csv_file='./data/txt_test.csv',\n",
        "        transform=transform_txt,\n",
        "    )\n",
        "\n",
        "    collate_batch = PadSequence(corpus_info.pad_idx)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "    val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "\n",
        "    for X, y in train_dataloader:\n",
        "        print(f\"Shape of X [B, N]: {X.shape}\")\n",
        "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "        break\n",
        "\n",
        "    return corpus_info, train_dataloader, val_dataloader, test_dataloader\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        return self.fc(embedded)\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_output, _ = self.lstm(embedded)\n",
        "        # Use the output of the last time step of the LSTM\n",
        "        lstm_output_last = lstm_output[:, -1, :]\n",
        "        return self.fc(lstm_output_last)\n",
        "\n",
        "\n",
        "def train_one_epoch(dataloader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 5\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (text, label) in enumerate(dataloader):\n",
        "        text, label = text.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| accuracy {:8.3f}\".format(\n",
        "                    epoch, idx, len(dataloader), total_acc / total_count\n",
        "                )\n",
        "            )\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(dataloader, model, criterion):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (text, label) in enumerate(dataloader):\n",
        "            text, label = text.to(device), label.to(device)\n",
        "            predicted_label = model(text)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc / total_count\n",
        "\n",
        "def main():\n",
        "    corpus_info, train_dataloader, val_dataloader, test_dataloader = get_data()\n",
        "\n",
        "    model = TextClassificationModel(corpus_info.vocab_size, EMBED_DIM, corpus_info.num_labels).to(device)\n",
        "    # model = LSTM(corpus_info.vocab_size, EMBED_DIM, HIDDEN_DIM, corpus_info.num_labels).to(device)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    #optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "    #TODO:\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "\n",
        "    # Initialize wandb run\n",
        "    #run = wandb.init(\n",
        "    #    name = \"text_classifier\", # Enter useful info to prevent confusions\n",
        "    #    reinit = True,\n",
        "    #    project = \"10-423-623 HW0\", # Runs under the same project can be plotted together\n",
        "    #)\n",
        "\n",
        "    total_accu = None\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train_one_epoch(train_dataloader, model, criterion, optimizer, epoch)\n",
        "        accu_val = evaluate(val_dataloader, model, criterion)\n",
        "        if total_accu is not None and total_accu > accu_val:\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            total_accu = accu_val\n",
        "        print(\"-\" * 59)\n",
        "        print(\n",
        "            \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
        "            \"valid accuracy {:8.3f} \".format(\n",
        "                epoch, time.time() - epoch_start_time, accu_val\n",
        "            )\n",
        "        )\n",
        "        #wandb.log({\"Accuracy\": accu_val})\n",
        "        print(\"-\" * 59)\n",
        "\n",
        "    # Evaluate some sample data\n",
        "    for idx, (text, label) in enumerate(test_dataloader):\n",
        "        text, label = text.to(device), label.to(device)\n",
        "        if idx == 0:\n",
        "            print('Sample test data: \\n')\n",
        "            print(text)\n",
        "            predicted_label = model(text)\n",
        "            print('Predicted label: ', predicted_label.argmax(1))\n",
        "            print('True label: ', label)\n",
        "\n",
        "    print(\"Checking the results of test dataset.\")\n",
        "    accu_test = evaluate(test_dataloader, model, criterion)\n",
        "    print(\"test accuracy {:8.3f}\".format(accu_test))\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"text_model.pth\")\n",
        "    #wandb.save('text_model.pth')\n",
        "    print(\"Saved PyTorch Model State to text_model.pth\")\n",
        "\n",
        "    # Finish your wandb run\n",
        "    #run.finish()"
      ],
      "metadata": {
        "id": "-yFlQe9XcauK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5448e2c6-4350-437f-b4e8-17abdaa70209"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qM9HC1C6gkW4",
        "outputId": "87e85f7a-1149-4dcc-d4f4-1b9c4b01ba9b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [B, N]: torch.Size([8, 1024])\n",
            "Shape of y: torch.Size([8]) torch.int64\n",
            "| epoch   1 |     5/   45 batches | accuracy    0.500\n",
            "| epoch   1 |    10/   45 batches | accuracy    0.375\n",
            "| epoch   1 |    15/   45 batches | accuracy    0.700\n",
            "| epoch   1 |    20/   45 batches | accuracy    0.450\n",
            "| epoch   1 |    25/   45 batches | accuracy    0.650\n",
            "| epoch   1 |    30/   45 batches | accuracy    0.700\n",
            "| epoch   1 |    35/   45 batches | accuracy    0.675\n",
            "| epoch   1 |    40/   45 batches | accuracy    0.775\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  0.38s | valid accuracy    0.724 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |     5/   45 batches | accuracy    0.771\n",
            "| epoch   2 |    10/   45 batches | accuracy    0.575\n",
            "| epoch   2 |    15/   45 batches | accuracy    0.725\n",
            "| epoch   2 |    20/   45 batches | accuracy    0.650\n",
            "| epoch   2 |    25/   45 batches | accuracy    0.750\n",
            "| epoch   2 |    30/   45 batches | accuracy    0.800\n",
            "| epoch   2 |    35/   45 batches | accuracy    0.775\n",
            "| epoch   2 |    40/   45 batches | accuracy    0.825\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  0.36s | valid accuracy    0.737 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |     5/   45 batches | accuracy    0.833\n",
            "| epoch   3 |    10/   45 batches | accuracy    0.625\n",
            "| epoch   3 |    15/   45 batches | accuracy    0.750\n",
            "| epoch   3 |    20/   45 batches | accuracy    0.775\n",
            "| epoch   3 |    25/   45 batches | accuracy    0.825\n",
            "| epoch   3 |    30/   45 batches | accuracy    0.875\n",
            "| epoch   3 |    35/   45 batches | accuracy    0.825\n",
            "| epoch   3 |    40/   45 batches | accuracy    0.825\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  0.38s | valid accuracy    0.816 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |     5/   45 batches | accuracy    0.896\n",
            "| epoch   4 |    10/   45 batches | accuracy    0.700\n",
            "| epoch   4 |    15/   45 batches | accuracy    0.775\n",
            "| epoch   4 |    20/   45 batches | accuracy    0.800\n",
            "| epoch   4 |    25/   45 batches | accuracy    0.900\n",
            "| epoch   4 |    30/   45 batches | accuracy    0.900\n",
            "| epoch   4 |    35/   45 batches | accuracy    0.850\n",
            "| epoch   4 |    40/   45 batches | accuracy    0.850\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  0.36s | valid accuracy    0.882 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |     5/   45 batches | accuracy    0.917\n",
            "| epoch   5 |    10/   45 batches | accuracy    0.800\n",
            "| epoch   5 |    15/   45 batches | accuracy    0.800\n",
            "| epoch   5 |    20/   45 batches | accuracy    0.850\n",
            "| epoch   5 |    25/   45 batches | accuracy    0.900\n",
            "| epoch   5 |    30/   45 batches | accuracy    0.900\n",
            "| epoch   5 |    35/   45 batches | accuracy    0.925\n",
            "| epoch   5 |    40/   45 batches | accuracy    0.925\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  0.37s | valid accuracy    0.895 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.910\n",
            "Saved PyTorch Model State to text_model.pth\n"
          ]
        }
      ]
    }
  ]
}